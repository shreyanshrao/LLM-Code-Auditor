# ğŸ§  LLM Code Auditor

An automated pipeline that transforms software requirements into code, tests it, analyzes quality, and generates an intelligent LLM-based feedback report.

## ğŸš€ Overview

This project simulates a **mini software development lifecycle**. Given a set of software requirements, it:
1. Converts them into user stories.
2. Designs modules and database schemas.
3. Auto-generates executable Python code.
4. Runs tests using `pytest` and performs static analysis with `pylint`.
5. Generates an LLM-based feedback report evaluating code quality and test coverage.

## ğŸ› ï¸ Technologies Used

- Python 3.10+
- `pytest` â€“ for test case execution
- `pylint` â€“ for static code analysis
- `openai` â€“ for LLM-based review and feedback
- OS & subprocess â€“ to automate code execution

---

## ğŸ“ Folder Structure

LLM-Code-Auditor/
â”‚
â”œâ”€â”€ requirements/ # Raw requirement and user story files
â”‚ â”œâ”€â”€ requirement.txt
â”‚ â””â”€â”€ user_stories.txt
â”‚
â”œâ”€â”€ design/ # Design documents
â”‚ â”œâ”€â”€ module_design.txt
â”‚ â””â”€â”€ db_design.txt
â”‚
â”œâ”€â”€ src/ # Auto-generated source code
â”‚ â””â”€â”€ module_code.py
â”‚
â”œâ”€â”€ tests/ # Test cases for modules
â”‚ â””â”€â”€ test_module_code.py
â”‚
â”œâ”€â”€ reports/ # Auto-generated reports
â”‚ â”œâ”€â”€ pylint_report.txt
â”‚ â”œâ”€â”€ test_results.txt
â”‚ â””â”€â”€ llm_feedback.txt
â”‚
â”œâ”€â”€ generate_code.py # Converts designs into source code
â”œâ”€â”€ run_tests.py # Executes test cases and saves output
â”œâ”€â”€ run_pylint.py # Executes pylint and saves output
â”œâ”€â”€ generate_feedback.py # Generates LLM feedback from test + lint
â””â”€â”€ README.md # Project explanation
